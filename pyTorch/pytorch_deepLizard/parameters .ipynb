{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Building Neural Network with pytorch</h1>\n",
    "<h3>Here model ==  network</h3>\n",
    "<b>main goal:</b> model or approximate function that maps image inputs to correct output class<br>\n",
    "The process to implement a NN below\n",
    "<ol>\n",
    "<li>Prepare the data</li>\n",
    "<li><b>Build the model</b></li>\n",
    "<li>Train the model</li>\n",
    "<li>Analyze the models results</li>\n",
    "</ol>\n",
    "Source with table and more info: https://deeplizard.com/learn/video/IKOHHItzukk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contains all the good stuff to make nn\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Extend Pytorch nn.Module Class</h3>\n",
    "create python class called Network<br>\n",
    "extend Pytorch's nn.Module class<br>\n",
    "define model layers:<br>\n",
    "CNN with convolution layers and linear layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Componets of class</h3>\n",
    "<i>Methods</i> (code)<br>\n",
    "<i>Attributes</i> (data)<br>\n",
    "good quick overview of classes: https://stats.stackexchange.com/questions/154798/difference-between-kernel-and-filter-in-cnn/188216"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network class below extends the nn.Module class\n",
    "\n",
    "Convolutional layers stride is not specified (defaults to (1,1))<br>\n",
    "    -stride is how far to move filter after each operation<br>\n",
    "    --(1,1)=> move one unit when moving right, one unit when moving down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):#extending nn.Module class\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        '''\n",
    "        Linear layers are fully connected\n",
    "        aka: dense, fully connected layers\n",
    "        Pytorch uses linear\n",
    "        '''\n",
    "        self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        #create output layer\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "    \n",
    "    def forward(self, t):\n",
    "        #still need implementation\n",
    "        return t "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Parameters</h2>\n",
    "parameters are place holders for a value, while\n",
    "<b>Arguments</b> are the actual value\n",
    "\n",
    "Convolutional layer (3 parameters):<br>\n",
    "<ul>\n",
    "<li>in_channels</li>\n",
    "<li>out_channels</li>\n",
    "<li>kernel_size</li>\n",
    "</ul>\n",
    "Linear layer (2 parameters):<br>\n",
    "<ul>\n",
    "<li>in_features</li>\n",
    "<li>out_features</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Hyperparameters</h3>\n",
    "values that are chose manual and arbitrarily<br>\n",
    "\n",
    "we choose hyperparameter values mainly based on <b>trial and error</b> and increasingly by <b>utilizing values</b> that have proven to <b>work well in the past</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Hyper parameters in CNNs</h4>\n",
    "these are the hyperparameters we use\n",
    "<ul>\n",
    "<li>kernel_size (sets the height and width)</li>\n",
    "<li>out_channels (sets depth of filter, # of kernels inside filter. One kernel produces one output channel)</li>\n",
    "<li>out_features (sets the size of the output tensor)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Dependent Hyperparameters</h3>\n",
    "these parameters values are depended on the data\n",
    "<ul>\n",
    "<li>in_channels (depends on number of color channels)</li>\n",
    "<li>out_features (depends on number of classes)</li>\n",
    "<li>in_features (depends on the output from prev layer)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>SIDENOTE: Kernel vs Filter</h4>\n",
    "<i>Kernel:</i> 2d tensor<br>\n",
    "<i>Filter</i> 3d tensor containing collection of kernels\n",
    "More info: https://stats.stackexchange.com/questions/154798/difference-between-kernel-and-filter-in-cnn/188216"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Learnable Parameters</h3>\n",
    "parameters whos <i>values are learned</i> during training<br><br>\n",
    "Generally we <i>start with <b>random values</b></i> that are <i>updated iteratively</i> as network learns<br><br>\n",
    "When a network is <b>learning</b> it is simply <b>updating learnable parameters</b> to find values that <i>minimize</i> the loss function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Make an instance of network class and inspect weights</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=192, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "  (out): Linear(in_features=60, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Accessing the Network's Layers</h4>\n",
    "For more info and possible customizations: https://deeplizard.com/learn/video/stWU37L91Yc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=120, out_features=60, bias=True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=60, out_features=10, bias=True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-1.7289e-01, -3.6889e-02, -2.2785e-02, -1.4952e-01,  1.9414e-01],\n",
       "          [ 2.1457e-02,  1.4025e-01, -2.9079e-02, -1.3774e-01,  1.3644e-01],\n",
       "          [ 1.2203e-01,  7.5122e-02,  3.5353e-02, -1.6008e-02,  1.7124e-01],\n",
       "          [-1.5510e-01, -6.4722e-02,  1.0430e-01, -1.1301e-01, -8.7242e-02],\n",
       "          [ 7.7930e-02, -1.6106e-01, -1.3908e-01, -1.0758e-01, -3.7452e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 5.2286e-02,  5.5730e-02, -7.9570e-02,  8.6542e-02,  2.2734e-03],\n",
       "          [-7.0686e-02,  1.8413e-01,  4.0059e-03, -1.4064e-01, -1.4402e-01],\n",
       "          [-4.4736e-02,  1.9903e-01,  1.2061e-01, -1.1757e-01,  1.8986e-01],\n",
       "          [ 5.9690e-02,  1.5190e-01,  1.3827e-01,  6.6847e-02,  2.3341e-02],\n",
       "          [-1.9387e-01, -7.4476e-03,  5.3592e-02, -1.1056e-01,  1.4097e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.7661e-01,  2.3881e-03, -4.2553e-03,  1.3229e-01, -4.7485e-02],\n",
       "          [ 9.2290e-02, -1.2766e-01,  1.9569e-01, -4.7710e-02, -1.8479e-01],\n",
       "          [ 9.9628e-02, -1.3248e-01, -4.1997e-02, -9.2522e-02, -1.6980e-02],\n",
       "          [-1.4496e-01,  1.4076e-01, -9.6472e-02, -4.0961e-02,  5.5131e-02],\n",
       "          [-3.6605e-02, -1.7294e-02,  1.6458e-01, -6.3550e-02,  3.4645e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 1.4065e-01,  1.5576e-01,  8.8707e-02,  7.7267e-02,  8.3004e-02],\n",
       "          [ 1.0446e-01,  3.0692e-02, -5.7880e-02,  1.3822e-01, -2.3645e-03],\n",
       "          [-1.9739e-01,  6.5690e-02,  2.7983e-02,  1.4113e-01, -6.5785e-02],\n",
       "          [-4.4401e-02, -9.4232e-02, -1.4454e-01,  1.6868e-02,  3.4941e-02],\n",
       "          [-1.1539e-01, -1.9206e-01,  1.2444e-01, -1.9300e-01,  4.7662e-02]]],\n",
       "\n",
       "\n",
       "        [[[-7.6330e-02,  1.9810e-01, -3.1994e-02, -1.1501e-01,  1.5812e-01],\n",
       "          [ 1.1271e-01, -3.5481e-02,  4.2429e-02,  1.2992e-01,  4.5053e-02],\n",
       "          [-2.9951e-02,  7.0457e-02,  4.3798e-02,  8.7645e-02,  1.9179e-01],\n",
       "          [-2.3778e-02, -1.9921e-01, -4.8934e-02, -1.0383e-01,  4.3684e-02],\n",
       "          [ 6.4537e-03,  1.4154e-01, -1.5152e-01,  9.2594e-02,  5.5776e-02]]],\n",
       "\n",
       "\n",
       "        [[[-8.5857e-02,  1.9098e-01,  1.7234e-01, -1.7373e-01, -1.7825e-01],\n",
       "          [ 1.7381e-01,  1.5559e-01,  1.9016e-01, -1.8073e-01, -4.2686e-02],\n",
       "          [ 1.4812e-01, -6.4604e-02, -1.3398e-01,  1.4486e-01,  1.0173e-01],\n",
       "          [-1.7346e-01, -9.0256e-02,  1.1436e-02,  8.1040e-02, -1.9213e-01],\n",
       "          [-5.6833e-02,  1.0511e-01,  6.7502e-02, -1.5650e-01,  7.4700e-05]]]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accessing Layer Weights\n",
    "network.conv1.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Weight Tensor shape</h4>\n",
    "CONVOLUTIONAL LAYERS\n",
    "<ol>\n",
    "<li>All filters are represented using a single tensor</li>\n",
    "<li>Filters have depth that accounts for the input channels(number of input channels being convolved)</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 5, 5])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see the shape \n",
    "#rank 4 weight tensor \n",
    "    #first axis has a length of 6 (6 filters)\n",
    "    #second axis length 1 (input channel)\n",
    "    #third and fourth axis (height and width)\n",
    "network.conv1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 6, 5, 5])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#notice input channel is 6 here\n",
    "network.conv2.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FULLY CONNECTED (LINEAR) LAYERS<br>Flattened rank-1 tensors as input and as output<br>\n",
    "<b>transform</b> the <b>in_features</b> to the <b>out_features</b> in a linear layer is by <b>using</b> a rank-2 tensor that is commonly called a <b>weight matrix</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: height = out_features, width = in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([120, 192])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fc1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 120])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fc2.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 60])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.out.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final note\n",
    "To see all the parameters/ weights of the network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in network.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in network.named_parameters():\n",
    "    print(name, 't\\t', param.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
